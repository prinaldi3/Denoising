{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Denoising.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOFSYyn8T54Bo7GIt3kus0+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prinaldi3/Denoising/blob/main/Denoising.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peRRRbMObUoJ"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWlg4ug7qYMf"
      },
      "source": [
        "Installations and Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzasYc9zDbfe"
      },
      "source": [
        "!python -m pip install pip==21.0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48hBL3MfbWNO"
      },
      "source": [
        "!rm -rf xca\n",
        "!git clone https://github.com/maffettone/xca"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSbo0JftbXZw"
      },
      "source": [
        "%%bash\n",
        "cd xca\n",
        "python -m pip install ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62uqgbhfuEnT"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhSTWjMGbYjl"
      },
      "source": [
        "#Importing from XCA package\n",
        "\n",
        "import xca\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from xca.ml.tf_models import build_CNN_encoder_model, build_CNN_decoder_model, VAE_denoising_training, VAE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_I986TmaDaQU"
      },
      "source": [
        "#Specify directories\n",
        "dataset_paths = ['/content/gdrive/MyDrive/BaTiO_training/BTO_sim.tfrecord'] #alter this to change the dataset you're training on\n",
        "out_dir = '/content/gdrive/MyDrive/BaTiO_training/training_1' #your training checkpoints will go here "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4U3_LMBBVs8i"
      },
      "source": [
        "#Build encoder/decoder networks using XCA methods\n",
        "\n",
        "out_dir = '/content/gdrive/MyDrive/BaTiO_training/training_1' #saves training checkpoints to this directory\n",
        "encoder, last_conv_shape = build_CNN_encoder_model(data_shape=(1750,1), \n",
        "                                                   latent_dim=100, \n",
        "                                                   dense_dims=[], \n",
        "                                                   filters=[32, 32, 16], \n",
        "                                                   kernel_sizes=[16, 16, 16], \n",
        "                                                   strides=[1, 1, 1], \n",
        "                                                   pool_sizes=[1, 1, 1], \n",
        "                                                   paddings=[\"same\"]*3, \n",
        "                                                   verbose=False) #change this to True to see Keras.summary() of model\n",
        "\n",
        "decoder = build_CNN_decoder_model(data_shape=(1750,1), \n",
        "                                  latent_dim=100, \n",
        "                                  last_conv_layer_shape=last_conv_shape, \n",
        "                                  filters = [16, 32, 32], \n",
        "                                  kernel_sizes=[16, 16, 16], \n",
        "                                  strides=[1, 1, 1], \n",
        "                                  paddings=[\"same\"]*4, \n",
        "                                  verbose=False) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cm1dIVIMbbca"
      },
      "source": [
        "#Build the VAE and train!\n",
        "\n",
        "kl_loss_factor = 0 #KL-Loss factor is set to 0 (we only care about reconstructions)\n",
        "vae = VAE(encoder, decoder, kl_loss_factor) \n",
        "vae.built=True\n",
        "\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-5, momentum=1e-5)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
        "\n",
        "res = VAE_denoising_training(vae,\n",
        "                            dataset_paths=dataset_paths,\n",
        "                            out_dir=out_dir,\n",
        "                            log_noise_min=log_noise_min,\n",
        "                            log_noise_max=log_noise_max,\n",
        "                            batch_size=64,\n",
        "                            multiprocessing=16,\n",
        "                            categorical=True,\n",
        "                            data_shape=(1750, 1),\n",
        "                            n_epochs=20,\n",
        "                            optimizer=optimizer,\n",
        "                            checkpoint_rate=100/20,\n",
        "                            verbose=True\n",
        "                            )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsEwLdDeuIGo"
      },
      "source": [
        "Plotting Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jAC7JGXVv4q"
      },
      "source": [
        "#Plot learning curves\n",
        "fig, axs = plt.subplots(1,2,figsize=(10,5))\n",
        "axs[0].plot(res[\"reconstruction_loss\"])\n",
        "axs[0].set_title(\"reconstruction Loss\")\n",
        "axs[1].plot(res[\"kl_loss\"])\n",
        "axs[0].set_ylim((0,40))\n",
        "axs[1].set_title(\"KL Loss\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCk2-GhTVzVn"
      },
      "source": [
        "from xca.ml.tf_data_proc import build_dataset\n",
        "\n",
        "#Build dataset\n",
        "log_noise_min= -1 #controls degree of noise\n",
        "log_noise_max= -.5\n",
        "\n",
        "def preprocess(data, label):\n",
        "    X = tf.cast(data, tf.float32)\n",
        "    X = (X - tf.math.reduce_min(X, axis=0, keepdims=True)) / (\n",
        "        tf.math.reduce_max(X, axis=0, keepdims=True)\n",
        "        - tf.math.reduce_min(X, axis=0, keepdims=True)\n",
        "    )\n",
        "    noisy = tf.cast(data, tf.float32) + tf.random.normal(\n",
        "        data.shape,\n",
        "        stddev=10 ** np.random.uniform(log_noise_min, log_noise_max),\n",
        "        dtype=tf.float32,\n",
        "    )\n",
        "    noisy = (noisy - tf.math.reduce_min(noisy, axis=0, keepdims=True)) / (\n",
        "        tf.math.reduce_max(noisy, axis=0, keepdims=True)\n",
        "        - tf.math.reduce_min(noisy, axis=0, keepdims=True)\n",
        "    )\n",
        "    return {\"X\": X, \"X_noisy\": noisy, \"label\": label}\n",
        "\n",
        "dataset, _ = build_dataset(\n",
        "    dataset_paths=dataset_paths,\n",
        "    batch_size=1,\n",
        "    multiprocessing=1,\n",
        "    categorical=True,\n",
        "    val_split=0.0,\n",
        "    data_shape=(1750,1),\n",
        "    # Preprocessing step adding noise and assuming probabilities needed on [0,1] and not on [-1,1]\n",
        "    preprocess=preprocess\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1-GkxVOV4OC"
      },
      "source": [
        "#Plot reconstructions\n",
        "\n",
        "for batch in dataset:\n",
        "  output = vae(batch[\"X_noisy\"], training=False)\n",
        "  X_denoise = output[\"reconstruction\"][0]\n",
        "  X = batch[\"X\"][0]\n",
        "  X_noisy = batch[\"X_noisy\"][0]\n",
        "  label = batch[\"label\"][0]\n",
        "  fig, ax = plt.subplots(figsize=(10, 10))\n",
        "  ax.plot(X_noisy+1.5, label=\"Raw\", color='gray')\n",
        "  ax.plot(X + .5, label=\"True\", color='k')\n",
        "  ax.plot(X_denoise+.5, 'r--', label=\"Denoise\")\n",
        "  ax.plot(abs(X-X_denoise), color='b', label=\"Residual\")\n",
        "  ax.set_title(\"Comparison of True and Denoised Patterns\")\n",
        "  ax.set_xlabel(\"Pixel #\")\n",
        "  ax.set_ylabel(\"Intensity [Arb.]\")\n",
        "  \n",
        "  plt.rc('font', size=20)          # controls default text sizes\n",
        "  plt.rc('axes', titlesize=30)     # fontsize of the axes title\n",
        "  plt.rc('axes', labelsize=30)    # fontsize of the x and y labels\n",
        "\n",
        "  ax.get_yaxis().set_ticks([])\n",
        "  #fig.savefig('/content/gdrive/MyDrive/poster_figs/batio_reconstruction_2', facecolor='white')\n",
        "  plt.legend()\n",
        "  break"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}